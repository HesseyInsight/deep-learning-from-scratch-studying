{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap3. word2vec\n",
    "\n",
    "## 3.1 추론 기반 기법과 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 벡터로 표현하는 방법은 활발히 연구가 되어왔다. 그 중에서 유명한 두 가지 기법이 있다. 바로 '통계 기반 기법'과 '추론 기반 기법'이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 기법이 단어의 의미를 얻는 방식은 크게 다르지만, 그 배경에는 분포 가설이 있다.<br>\n",
    "앞 장에서 '분포 가설'이란 단어의 의미가 단어 주변의 단어에 의해 결정된다고 정의했었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이번 장에서는\n",
    "    * 통계 기반 기법의 문제 제시\n",
    "    * 추론 기반 기법의 이점 제시\n",
    "    * word2vec의 전처리를 위해 신경망으로 '단어'를 처리하는 예제를 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 통계 기반 기법의 문제점 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현했다.<br>\n",
    "구체적으로 co-occurence matrix를 만들고 그 행렬에 SVD를 적용하여 밀집벡터(단어의 분산 표현)을 얻었다.<br>\n",
    "이러한 통계 기반 기법은 현업에서 대규모 말뭉치를 다룰 때 문제가 발생한다.<br>\n",
    "SVD를 nxn 행렬에 적용하는 비용은 O(n***3)이다.<br>\n",
    "반면 추론 기반 기법에서는 미니배치로 학습하는 것이 일반적이다.<br>\n",
    "미니배치 학습에서는 신경망이 한 번에 소량(미니배치)의 학습 샘플씩 반복해서 학습하며 가중치를 갱신한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 추론 기반 기법 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 기반 기법에서는 당연히 '추론'이 주된 작업이다.<br>\n",
    "'맥락(context)'을 입력하면 모델은 각 단어의 출현 확률을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "추론 기반 기법도 통계 기반 기법처럼 분포 가설에 기초한다.<br>\n",
    "분포 가설이란 '단어의 의미는 주변 단어에 의해 형성된다'는 가설로, 추론 기반 기법에서는 이를 앞에서와 같은 추측 문제로 귀결시켰다.<br>\n",
    "이처럼 두 기법 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구 주제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 신경망에서의 단어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"지금부터는 신경망을 이용해 '단어'를 처리한다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩(One-hot Encoding)을 통해 단어를 고정 길이 벡터로 변환하면 우리 신경망의 입력층은 뉴런의 수를 '고정'할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩을 통해 단어를 벡터로 표현할 수 있게 되었고, 벡터 처리를 통해 신경망의 '계층(layer)'를 표현할 수 있게 되었다. 다시말해 단어를 신경망으로 처리할 수 있게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69539145  0.34287175  0.82276371]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0])  # 입력\n",
    "W = np.random.randn(7, 3)  # 가중치\n",
    "h = np.matmul(c, W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드는 단어 ID가 0인 단어를 원핫 표현으로 표현한 다음 완전연결계층을 통과시켜 변환하는 모습을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.56103313 -1.22702894  1.02625986]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
