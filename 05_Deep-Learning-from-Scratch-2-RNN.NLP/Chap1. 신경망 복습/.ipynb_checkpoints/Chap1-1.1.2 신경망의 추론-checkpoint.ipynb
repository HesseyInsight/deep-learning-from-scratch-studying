{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 추론\n",
    "신경망에서 수행하는 작업은 두 단계로 나눌 수 있다. 이번 절에서는 '추론'에 집중해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전연결계층에 의한 변환\n",
    "# 미니배치를 적용한 예시\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(2, 4)  # 가중치\n",
    "b1 = np.random.randn(4)     # 편향\n",
    "x = np.random.randn(10, 2)  # 입력\n",
    "\n",
    "# 입력노드가 2개일 때, 4개의 출력노드를 갖는 신경망\n",
    "# (10, 2) -> (10, 2) x (2, 4) + (4) = (10, 4)\n",
    "\n",
    "h = np.matmul(x, W1) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "[[ 0.96693241 -0.02815344]\n",
      " [ 1.34707481 -0.46435713]\n",
      " [ 0.60334194  0.92355497]\n",
      " [-1.22126792 -0.1603039 ]\n",
      " [-0.04535196  0.67409964]\n",
      " [-0.86934708  0.45223215]\n",
      " [-0.49676093  0.00206163]\n",
      " [ 0.56662034 -1.52395026]\n",
      " [-0.04539973 -0.09461   ]\n",
      " [ 0.96083142 -0.17576389]]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.40998083 -0.49736758 -1.10430266 -0.09364953]\n",
      " [-2.66759939 -0.4793049  -0.3929734  -0.72038846]\n",
      " [-2.36911973 -0.48202799 -3.22141159  1.11786253]\n",
      " [ 0.08932306 -0.76262406  1.9057381   0.48741976]\n",
      " [-1.54733631 -0.57349504 -1.76384308  1.04932676]\n",
      " [-0.54000855 -0.68388286 -0.16774919  1.07105614]\n",
      " [-0.78380111 -0.66756058  0.59029107  0.43085266]\n",
      " [-1.38684507 -0.63569432  3.40213664 -1.67252596]\n",
      " [-1.25167653 -0.6204084   0.30253843  0.16908045]\n",
      " [-2.34639071 -0.50709207 -0.70011763 -0.26063872]]\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 완전연결계층에 의한 변환은 '선형 변환'이다.\n",
    "\n",
    "여기에 **'비선형 효과'**를 부여하는 것이 바로 **활성화함수(Activation Function)**이다.\n",
    "\n",
    "비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있다. 이번 예제에서는 시그모이드 함수(Sigmoid Function)을 활성화 함수로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Sigmoid](https://www.researchgate.net/profile/Knut_Kvaal/publication/239269767/figure/fig2/AS:643520205430784@1530438581076/An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 임의의 실수를 입력받아 0에서 1사이의 실수를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시그모이드 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08241477, 0.3781595 , 0.24893457, 0.47660471],\n",
       "       [0.06491253, 0.38241628, 0.40300172, 0.32730745],\n",
       "       [0.08555798, 0.38177336, 0.03836787, 0.75359202],\n",
       "       [0.52231593, 0.31807683, 0.87053959, 0.61949841],\n",
       "       [0.17547132, 0.36043075, 0.14630967, 0.7406456 ],\n",
       "       [0.36818559, 0.33539524, 0.45816077, 0.74479771],\n",
       "       [0.31350124, 0.33904328, 0.64343193, 0.60607726],\n",
       "       [0.1999119 , 0.34622049, 0.96777124, 0.15808769],\n",
       "       [0.22241006, 0.34968857, 0.57506294, 0.5421697 ],\n",
       "       [0.08735309, 0.37587546, 0.33178615, 0.4352067 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sigmoid(h)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 10개의 데이터를 갖는 2개의 입력노드가있다. 은닉층을 거쳐서 3개의 출력 노드를 갖는 완전신경망을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51795082 -1.75524138 -3.89433186]\n",
      " [-0.3352913  -0.35151051 -4.36980344]\n",
      " [ 0.25931231 -0.09451576 -3.28232598]\n",
      " [-0.10334649 -1.87849782 -2.80223497]\n",
      " [ 0.44216298 -0.53821412 -2.3901712 ]\n",
      " [-0.65190302 -0.72343581 -4.68629951]\n",
      " [ 0.08865513 -0.78720629 -3.26749759]\n",
      " [-0.58274662 -1.72390954 -4.2648517 ]\n",
      " [ 0.28193041 -0.16954535 -3.18084949]\n",
      " [ 0.03755955  0.14269063 -4.03761042]]\n",
      "[[-0.51795082 -1.75524138 -3.89433186]\n",
      " [-0.3352913  -0.35151051 -4.36980344]\n",
      " [ 0.25931231 -0.09451576 -3.28232598]\n",
      " [-0.10334649 -1.87849782 -2.80223497]\n",
      " [ 0.44216298 -0.53821412 -2.3901712 ]\n",
      " [-0.65190302 -0.72343581 -4.68629951]\n",
      " [ 0.08865513 -0.78720629 -3.26749759]\n",
      " [-0.58274662 -1.72390954 -4.2648517 ]\n",
      " [ 0.28193041 -0.16954535 -3.18084949]\n",
      " [ 0.03755955  0.14269063 -4.03761042]]\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Fully-connected Layer\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h1 = np.matmul(x, W1) + b1\n",
    "a1 = sigmoid(h1)\n",
    "h2 = np.matmul(a1, W2) + b2\n",
    "print(h2)\n",
    "\n",
    "h1_dot = np.dot(x, W1) + b1\n",
    "a1_dot = sigmoid(h1_dot)\n",
    "h2_dot = np.dot(a1_dot, W2) + b2\n",
    "print(h2_dot)\n",
    "\n",
    "print(h2 == h2_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot()내적으로 계산한 값이나 np.matmul() matrix multiplication으로 계산한 값이나 결과는 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "print(h2_dot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**해석**<br>\n",
    "입력 x의 형상은 (10, 2)이다. 이는 2차원 데이터 10개가 미니배치로 처리된다는 뜻이다.<br>\n",
    "최종 출력인 s의 형상은 (10, 3)이 된다. 10개의 데이터가 한꺼번에 처리되어(미니배치) 3차원 데이터로 변환되었다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 위의 신경망은 3차원 데이터를 출력한다. 따라서 각 차원의 값을 이용하여 3클래스 분류를 할 수 있다.<br>\n",
    "이 경우, 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 '점수(score)'가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 계층으로 클래스화 및 순전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 장에서는 신경망에의 처리를 계층(layer)로 구현해본다.<br>\n",
    "Affine 계층, Sigmoid 계층 등을 구현한다.<br>\n",
    "각 계층은 **파이썬 클래스로 구현**하며 기본 변환을 수행하는 메서드의 이름은 forward()로 한다.<br/>\n",
    "\n",
    "* 모든 계층은 forward()와 backward() 메서드를 가진다.\n",
    "* 모든 계층은 인스턴스 변수인 params와 grads를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-forward()와 backward()는 각각 순전파와 역전파를 수행한다.<br>\n",
    "-params는 가중치와 편향 같은 매개변수를 담는 리스트이다.<br>\n",
    "-grads는 params에 저장된 각 매개변수에 대응하여, 해당 매개변수의 기울기를 보관하는 리스트이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순전파 구현하기\n",
    "이번 절에서는 순전파만 구현할 것이므로 앞서 정의한 규칙 중 두 사항만 적용한다.<br>\n",
    "* 첫째, 각 계층은 forward()메서드만 가진다.\n",
    "* 둘째, 매개변수들은 params인스턴스 변수에 보관한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1. 시그모이드 함수 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현하기\n",
    "# 우선 Sigmoid계층부터 구현을 한다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []  # Sigmoid 함수에서는 학습할 매개변수가 없기 때문에 빈 리스트가 저장된다.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 우리는 Sigmoid 함수를 클래스로 구현했다<br>\n",
    "주 반환 처리는 forward(x)메서드가 담당한다.<br>\n",
    "한편 Sigmoid 계층에서는 학습하는 매개변수가 따로 없으므로 인스턴스 변수인 params는 빈 리스트로 초기화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2. Affine 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]  # Affine계층은 가중치와 편향을 매개변수로 받는다.\n",
    "                              # list인 params 인스턴스 변수에 가중치와 편향이 저장된다.\n",
    "        \n",
    "    def forward(self, x):  # forward()메서드는 순전파 처리를 구현한다.\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 예제에서는 입력 x가 Affine계층 -> Sigmoid계층 -> Affine 계층을 차례로 거쳐 점수인 s를 얻게 된다.<br>\n",
    "이 신경망을 TwoLayerNet이라는 클래스로 추상화하고, 주 추론 처리는 predict(x)메서드로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](2-layer-network.PNG \"TwoLayerNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로는 위의 그림처럼 계층 관점에서 신경망을 표현하며 구현할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3. TwoLayerNet 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \n",
    "    # 초기화 메서드\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 1. 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)  # (I, H)\n",
    "        b1 = np.random.randn(H)     # (H)\n",
    "        W2 = np.random.randn(H, O)  # (H, O)\n",
    "        b2 = np.random.randn(O)     # (O)\n",
    "        \n",
    "        # 2. 계층 생성\n",
    "        # 3개의 계층을 생성한다. \n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 3. 모든 가중치를 리스트에 모은다.\n",
    "        # 학습해야할 가중치 매개변수들을 params 리스트에 저장한다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "    \n",
    "    # 추론 메서드 predict(x)\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드와 엣지로 부터 시작된 신경망을 계층으로 표현하고, 계층을 쌓아서 이렇게 아름다운 layer를 쌓을 수 있구나! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet 클래스를 이용해 신경망의 추론을 수행해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.493504  ,  0.30873184, -0.45831549],\n",
       "       [ 2.40062751,  0.35734057, -0.41455773],\n",
       "       [ 2.56094171,  0.30904921, -0.49435139],\n",
       "       [ 2.43633048, -0.18866437, -0.36408759],\n",
       "       [ 2.74911524,  0.28769381, -0.59453653],\n",
       "       [ 2.07424602,  0.73271601, -0.30743365],\n",
       "       [ 2.26458466,  0.54492544, -0.37943449],\n",
       "       [ 2.73110202,  0.12851387, -0.58847229],\n",
       "       [ 2.34487776,  0.33967927, -0.26962542],\n",
       "       [ 2.4008548 ,  0.24383621, -0.38656985]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.92936313,  1.81592209,  0.46067534],\n",
       "       [ 2.16202546,  1.8668982 ,  0.76216333],\n",
       "       [ 2.49347178,  2.34648163,  1.08957452],\n",
       "       [ 2.85151413,  2.44173226,  1.02630831],\n",
       "       [ 1.90330748,  2.24290409, -0.16376717],\n",
       "       [ 2.68528544,  2.4434506 ,  1.08576007],\n",
       "       [ 1.58891617,  1.58158574,  0.38762407],\n",
       "       [ 1.97411161,  2.07629872,  0.25089728],\n",
       "       [ 1.95862107,  1.81930577,  0.50127526],\n",
       "       [ 2.07381279,  1.85443573,  0.65318318]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out\n",
    "\n",
    "    \n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):  # input node의 사이즈, hidden node의 사이즈, output node의 사이즈로 생각해보면 어떨까?\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        #1. 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        #2. 3개의 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        #3. 학습해야 할 가중치 리스트 저장\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "    \n",
    "    # 추론 메서드\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging하는데 애를썼네...;;<br>\n",
    "디버깅은 Visual Studio를 통해 해야겠군...!(공부해야할게 하나 추가되었다...하하!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
