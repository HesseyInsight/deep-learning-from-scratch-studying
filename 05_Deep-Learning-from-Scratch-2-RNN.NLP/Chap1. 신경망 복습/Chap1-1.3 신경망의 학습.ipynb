{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 신경망의 학습\n",
    "신경망으로 좋은 추론을 하기 위해서는 학습을 먼저 수행하고, 학습된 매개변수를 이용해 추론을 수행하는 흐름이 일반적이다.\n",
    "\n",
    "한편 신경망의 학습은 최적의 매개변수값을 찾는 작업이다.\n",
    "\n",
    "이번절에서는 신경망의 학습에 대해 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에는 학습이 얼마나 잘 이루어지고 있는지를 나타내기 위한 '척도'가 필요하다.<br>\n",
    "일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 **손실(Loss)**를 사용한다.<br>\n",
    "학습시 주어진 정답 데이터와 신경망이 예측한 결과를 비교해 예측이 얼마나 나쁜가를 산출한 단일 스칼라값(loss)를 구할 수 있다.\n",
    "\n",
    "\n",
    "* 학습의 척도로 Loss를 사용한다.\n",
    "* Loss는 정답 데이터와 신경망 예측 결과를 비교한 단일 스칼라값으로 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 손실은 **손실 함수(loss function)**을 사용해서 구한다.\n",
    "\n",
    "다중 클래스 분류(multi-class classification) 신경망에서는 손실함수로 흔히 **교차 엔트로피 오차(Cross Entropy Error)**를 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**손실함수를 적용한 신경망 계층 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/TwoLayerNetwork-with-LossFunction.PNG \"TwoLayerNet-with-LossFunction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-X: 입력데이터<br>\n",
    "-t: 정답레이블<br>\n",
    "-L: 손실<br>\n",
    "-Softmax계층의 출력: 확률<br>\n",
    "-Cross Entropy Error 계층: Softmax 계층 출력 확률과 정답 레이블이 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax 함수**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Softmax-function.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax함수 출력의 각 원소는 0.0이상 1.0이하의 실수이다. <br>\n",
    "* 원소들을 모두 더하면 1.0이 된다.\n",
    "* 이 때문에 Softmax의 출력을 '확률'로 해석할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n",
      "74.1221542101633\n",
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a)\n",
    "print(exp_a)\n",
    "\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "print(sum_exp_a)\n",
    "\n",
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 함수 정의\n",
    "\n",
    "def softmax(s):\n",
    "    c = np.max(s)\n",
    "    exp_s = np.exp(s-c)\n",
    "    sum_exp_s = np.sum(exp_s)\n",
    "    y = exp_s / sum_exp_s\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "s = np.array([0.3, 2.9, 4.0])\n",
    "\n",
    "print(softmax(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax 함수의 특징\n",
    "    * 함수의 출력은 0에서 1.0사이이다.\n",
    "    * 함수 출력의 총 합은 1이다.\n",
    "    * Softmax함수를 적용해도 각 원소의 대소 관계는 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CEE(Cross Entropy Error)**<br>\n",
    "![alt text](./img/Cross-Entropy.PNG \"Cross-Entropy function\")\n",
    "* tk는 k번째 클래스에 해당하는 정답 레이블이다.<br>\n",
    "* log는 네이피어 상수(혹은 오일러의수)e를 밑으로 하는 로그이다.<br>\n",
    "* 정답 레이블은 t=[0, 0, 1]과 같이 원-핫 벡터로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minibatch를 적용한 CrossEntropyError**<br>\n",
    "![alt text](./img/Minibatch-CrossEntropy.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tnk는 n번째 데이터에서 k차원째의 값을 의미한다.<br>\n",
    "* ynk는 신경망의 출력이고, tnk는 정답 레이블이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax-with-Loss Layer**<br>\n",
    "![alt text](./img/Softmax-with-Loss-Layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 책에서는 소프트맥스 함수와 교차 엔트로피 오차를 계산하는 계층을 Softmax with Loss 계층 하나로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 미분과 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것이다.\n",
    "\n",
    "이 때 중요한 것이 바로 **'미분'**과 **'기울기'**이다.\n",
    "\n",
    "이번 절에서는 미분과 기울기에 대해 간략히 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient(기울기)란 무엇인가\n",
    "-x에 관한 y의 미분은 dy/dx라고 쓴다<br>\n",
    "-이것이 의미하는 것은 x를 조금 변화시켰을 때(조금의 변화를 극한까지 줄일 때) y값이 얼마나 변하는가이다.\n",
    "\n",
    "* 두 가지의 Gradient\n",
    "    * 벡터의 각 원소에 대한 미분을 정리\n",
    "    * 행렬에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 벡터의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L은 스칼라, x는 벡터인 함수 L = f(x) 가 있다.\n",
    "\n",
    "이 때 xi(x의 i번 째 원소)에 대한 미분과 x의 다른 원소의 미분을 다음과 같이 정리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/gradient-vector.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 벡터의 각 원소에 대한 미분을 정리한 것이 기울기이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행렬의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/gradient-matrix.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행렬의 기울기는 원래의 행렬과 형상이 같다.\n",
    "  \n",
    "이 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 시 신경망은 학습데이터를 통해 손실(Loss)을 출력한다. 이 때 우리가 알고 싶은 것은 바로 **매개변수에 대한 손실의 기울기**이다.\n",
    "\n",
    "Gradient of parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 연쇄 법칙(Chain-Rule)\n",
    "\n",
    "오차역전파법을 이해하는 열쇠는 **연쇄법칙(Chain-Rule)**이다. 연쇄법칙이란 합성함수에 대한 미분의 법칙이다.\n",
    "\n",
    "* 연쇄법칙이 중요한 이유\n",
    "\n",
    "아무리 복잡하고, 많은 함수를 사용하더라도 개별 미분들을 이용해 전체 미분값을 구할 수 있기 때문이다.<br>\n",
    "즉, 각 함수의 국소적 미분을 구할 수 있다면 그 값들을 곱해서 전체 미분을 구할 수 있다.\n",
    "\n",
    "* Chain-Rule을 활용한 역전파 미분값 구하기\n",
    "\n",
    "Chain-Rule에 따르면 역전파로 흐르는 미분값은 상류로부터 흘러온 미분과 각 연산 노드의 국소적인 미분을 곱해 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기노드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/분기노드.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분기노드는 위의 그림과 같이 분기하는 노드이다.\n",
    "\n",
    "분기노드의 역전파는 (상류노드에서 흘러들어온) 분기노드의 합이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개의 분기 노드를 일반화 하면 N개의 분기가 된다. 이를 Repeat Node라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Repeat-Node.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분기노드 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D)  # 입력\n",
    "# y = np.random.randn(8, 7)\n",
    "y = np.repeat(x, N, axis=0)  # 순전파\n",
    "\n",
    "# 역전파\n",
    "dy = np.random.randn(N,D)\n",
    "dx = np.sum(dy, axis=0, keepdims=True)  # Keppdims= Ture\n",
    "                                        # If this is set to True, the axes which are reduced are left\n",
    "                                        # in the result as dimensions with size one. With this option,\n",
    "                                        # the result will broadcast correctly against the input array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Sum-Node.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum Node는 범용 덧셈 노드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum node 구하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(D, N)\n",
    "y = np.sum(x, axis=0, keepdims=True)  # keepdims=True를 통해 2차원 배열의 차원수를 유지한다.\n",
    "\n",
    "dy = np.random.randn(1, D)\n",
    "dx = np.repeat(dy, N, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Minibatch-MatMul-Shape.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul Node 구현하기\n",
    "이 노드를 하나의 계층으로 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        \n",
    "        self.grads[0][...] = dW  # 얕은 복사냐 깊은 복사냐? 둘 다 무슨 말이야??\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라인 18번 째 주석, 얕은 복사?? 깊은 복사??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 상태에서 a=b와 a[...]=b 모두 a에는 [4, 5, 6]이 할당된다.\n",
    "\n",
    "그러나 두 경우에 a가 가리키는 메모리의 위치는 서로 다르다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/얕은복사-깊은복사.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = b에서는 a가 가리키는 메모리 위치가 b가 가리키는 위치와 같아진다. \n",
    "\n",
    "실제 데이터(4, 5, 6)는 복사되지 않는다는 뜻으로 이를 '얕은 복사'라고 한다.\n",
    "\n",
    "한편 a[...]=b일 때는 a의 메모리 위치는 변하지 않고, 대신 a가 가리키는 메모리에 b의 원소가 복제된다. \n",
    "\n",
    "실제 데이터가 복제된다는 뜻에서 깊은 복사라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'생략 기호'를 이용하여 변수의 메모리 주소를 고정할 수 있다.\n",
    "\n",
    "이처럼 메모리 주소를 고정함으로써 인스턴스 변수 grads를 다루기 더 쉬워진다.\n",
    "\n",
    "* grads list\n",
    "    * grads list는 각 매개변수의 기울기를 저장한다.\n",
    "    * list의 각 원소는 NumPy 배열이며, 계층을 생성할 때 한 번만 생성한다.\n",
    "    * 항상 '생략기호'를 이용하므로 NumPy 배열의 메모리 주소가 변하는 일 없이 항상 값을 덮어쓴다.\n",
    "    * 이렇게 하면 기울기를 그룹화 하는 작업을 최초에 한 번만 하면 된다는 이점이 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.5 기울기 도출과 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 절에서는 Sigmoid 계층, 완전연결 계층의 Affine 계층, Softmax with Loss 계층을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax with Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 함수와 교차 엔트로피 오차는 Softmax with Loss라는 하나의 계층으로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Softmax-with-Loss-Layer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 계산그래프에서 Softmax Function은 Softmax Layer로, Cross Entropy Error는 Cross Entropy Layer로 표기했다.\n",
    "\n",
    "3-클래스 분류를 가정하여 이전 계층으로부터 3개의 입력을 받도록 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Error계층은 Softmax의 출력 (y1, y2, y3)와 정답 레이블 (t1, t2, t3)를 받고 이 데이터로 손실(Loss) L 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SoftmaxWithLoss계층 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax 계층 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        c = np.max(x)\n",
    "        exp_s = x-c\n",
    "        sum_exp_s = np.sum(exp_s, axis=0)\n",
    "        self.out = exp_s / sum_exp_s\n",
    "        '''\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "        # 정답 레이블이 원핫 벡터일 정우 정답의 인덱스로 변환??\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "            \n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "와... 이건 좀 어려운데...;;  -20.03.10.Tue. pm 6:10 -\n",
    "\n",
    "==> [밑바닥부터 시작하는 딥러닝 1권] Appendix-A: Softmax-with-Loss 계층의 계산 그래프 공부합시다...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.6 가중치 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차역전파법으로 구한 기울기는 어디에 사용되는걸까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망 학습 순서\n",
    "    * 1단계: 미니 배치\n",
    "    * 2단계: 기울기 계산\n",
    "    * 3단계: 매개변수 갱신\n",
    "    * 4단계: 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차역전파법을 통해 각 매개변수에 대한 손실함수의 기울기를 계산한다 -> 기울기를 통해 어떤 매개변수가 손실에 영향을 많이 주는지 알 수 있다?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오차역전파법을 통해 구한 매개변수에 대한 손실함수의 기울기\n",
    "    * 이 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킨다.\n",
    "    * 따라서 **매개변수를 그 기울기와 반대 방향으로 갱신**하면 손실을 줄일 수 있다.\n",
    "    * 이것이 바로 **경사 하강법**(Gradient Descent)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/Gradient-Descent.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
