{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep-Learning-from-scratch\n",
    "## Chap5. Backpropagation\n",
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 절에서 구현한 계층을 조합하면 마치 레고 블록을 조립하듯 신경망을 구축할 수 있다.<br/>\n",
    "이번 절에서는 지금까지 구현한 계층을 조합해서 신경망을 구축해보자.<br/>\n",
    "레고 블록을 조립하듯 구축할 수 있다는 소식은 좋았으나... 뒤에서 난적들을 마주하게 되는데..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "* 신경망의 학습 순서를 다시한번 복습해보자\n",
    "    * 전제\n",
    "        * 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 신경망의 학습은 다음 4단계로 수행된다.\n",
    "    * 1단계 - 미니배치\n",
    "        * 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "    * 2단계 - 기울기 산출\n",
    "        * 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "    * 3단계 - 매개변수 갱신\n",
    "        * 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "    * 4단계 - 반복\n",
    "        * 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다시한번 강조하지만 학습이란 결국 '손실함수'를 줄이는 것이고,\n",
    "* 손실함수를 가장 작게 만드는 최적의 파라메터를 구하는 과정이다.\n",
    "1. 미니배치를 통해 데이터를 무작위로 가져온다. <br/>\n",
    "2. 기울기를 구해 손실 함수의 값을 가장 작게 만드는 방향을 파악한다.<br/>\n",
    "3. 가중치 매개변수를 기울기가 제시하는 방향으로 아주 조금 갱신한다.\n",
    "4. 위의 3단계 과정을 반복하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드디어! 본격적인 구현이다. (드디어 계산그래프를 신경망에 적용한 데 이어 드디어 본격적으로 구현을 한다!!) <br/>\n",
    "본 절에서는 2층 신경망을 TwoLaylerNet 클래스로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TwoLayerNet 클래스의 인스턴스 변수\n",
    "* Params\n",
    "    * 딕셔너리 변수로, 신경망의 매개변수를 보관한다.\n",
    "    * params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향\n",
    "* Layers\n",
    "    * 순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관한다. (* 순서가 있다)\n",
    "    * layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지한다.\n",
    "* lastLayer\n",
    "    * 신경망의 마지막 계층이다.\n",
    "    * 본 예제에서는 SoftmaxWithLoss 계층을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TwoLayerNet 클래스의 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __init__(self, input_size, hidden_size, output_size, weight_init_std)\n",
    "    * 초기화를 수행한다. \n",
    "    * (인수는)앞에서부터 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가증치 초기화시 정규분포의 스케일\n",
    "* predict(self, x) \n",
    "    * 예측(추론)을 수행한다.\n",
    "    * 인수 x는 이미지 데이터\n",
    "* loss(self, x, t)\n",
    "    * 손실 함수의 값을 구한다.\n",
    "    * 인수 x는 이미지 데이터, t는 정답 레이블\n",
    "* accuracy(self, x, t) \n",
    "    * 정확도를 구한다.\n",
    "* numericcal_gradient(self, x, t)\n",
    "    * 가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.(앞 장에서 다뤘던 내용과 같다)\n",
    "* gradient\n",
    "    * 가중치 매개변수의 기울기를 오차역전파법으로 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오차역전파법을 적용한 신경망을 구현해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master')\n",
    "import numpy as np\n",
    "\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict  # 순서가 있는 dictionary?\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):  # 입력층의 뉴런수, 은닉층 뉴런수, 출력층의 뉴런수, \n",
    "                                                                                     # 가중치 초기화 시 정규분포의 스케일\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()  # layer(계층)의 경우 순서가 있는 dictionary로 저장해준다\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        # latLayer\n",
    "        self.lastLayer = SoftmaxWithLoss()  #신경망의 마지막 계층은 softmaxwithloss 계층으로 구현한다.\n",
    "        \n",
    "    def predict(self, x):  # 입력 데이터(이미지) x를 가지고 예측(추론)을 수행한다.\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)  # lambda?\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm...지금까지 잘 따라왔었는데..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 기울기를 구하는 방법을 두 가지 설명했다. <br/>\n",
    "하나는 수치 미분을 써서 구하는 방법, 또 하나는 해석적으로 수식을 풀어 구하는 방법이다. <br/>\n",
    "후자인 해석적으로 수식을 풀어 구하는 방법은 오차역전파법을 이용하여 매개변수가 많아도 효율적으로 계산할 수 있다. <br/>\n",
    "이제부터는 효율적이고 계산속도가 빠른 오차역전파법(backpropagation)을 사용하도록 한다.<br/>\n",
    "그렇다면 수치미분은 필요 없는 걸까? 수치미분은 backprop이 제대로 구현했는지 확인하기 위해 필요하다. <br/>\n",
    "이처럼 두 방식으로 구한 기울기가 일치함(엄밀히 말하면 거의 같음)을 확인하는 작업을 **기울기 확인(gradient check)**라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient check는 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.905945456311921e-10\n",
      "b1:3.0680023546040927e-09\n",
      "W2:5.445617807660519e-09\n",
      "b2:1.4079563401964146e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master')\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# 여기에 위에서 구현한 TowLayerNet을 사용한다.\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다는 것을 알 수 있다.<br/>\n",
    "예를들어 1번째 층의 편향 오차는 4.905945456311921e-10(0.0000000004905945456311921)이다.<br/>\n",
    "이로써 오차역전파법으로 구한 기울기도 올바름이 드러나면서 실수 없이 구현했다는 믿음이 커지는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
