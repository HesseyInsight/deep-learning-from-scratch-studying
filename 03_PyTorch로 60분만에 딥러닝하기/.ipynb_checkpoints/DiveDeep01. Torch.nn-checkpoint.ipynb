{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiveDeep into PyTorch\n",
    "TutorialÏùÑ Îî∞ÎùºÌïòÎ©¥ÏÑú Î∞úÍ≤¨Ìïú Í∂ÅÍ∏àÏ¶ùÏùÑ Ìï¥ÏÜåÌïòÍ≥º Ìï¥Í≤∞ÌïòÏßÄ Î™ªÌïú Î¨∏Ï†úÎì§ÏùÑ Ìï¥Í≤∞ÌïòÎäî ÏãúÍ∞Ñ\n",
    "\n",
    "PyTorch DocÏùÑ ÌÜµÌï¥ ÌïòÎÇòÌïòÎÇò ÎèÖÌååÌï¥ ÎÇòÍ∞ÄÏûê!\n",
    "\n",
    "Torch Documentation: https://pytorch.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Q1. How to calculate the output size after Conv2d in PyTorch? -20.02.20.Thur pm4:20-<br>\n",
    "Link: http://bitly.kr/bbjdFjRp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Q2. Linear layer inpt neurons number calculation after conv2d -20.02.20.Thur pm 4:20-<br>\n",
    "Link: http://bitly.kr/bH5wNhWr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Torch.nn\n",
    "Ref: https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module\n",
    "CLASS<code>torch.nn.Module</code><br>\n",
    "\n",
    "Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowwing to nest them in a tree structure. you can assign the submodules as regular attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class <code>torch.nn.Conv2d()</code>\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stiride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button type=\"button\">Definition</button><br> Applies a 2D convolution over an input signal composed of several input planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïó¨Îü¨ ÌèâÎ©¥ÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßÑ ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D convolutionÏùÑ Ï†ÅÏö©ÌïúÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in_channels (python:int) ‚Äì Number of channels in the input image\n",
    "\n",
    "* out_channels (python:int) ‚Äì Number of channels produced by the convolution\n",
    "\n",
    "* kernel_size (python:int or tuple) ‚Äì Size of the convolving kernel\n",
    "\n",
    "* stride (python:int or tuple, optional) ‚Äì Stride of the convolution. Default: 1\n",
    "\n",
    "* padding (python:int or tuple, optional) ‚Äì Zero-padding added to both sides of the input. Default: 0\n",
    "\n",
    "* padding_mode (string, optional) ‚Äì zeros\n",
    "\n",
    "* dilation (python:int or tuple, optional) ‚Äì Spacing between kernel elements. Default: 1\n",
    "\n",
    "* groups (python:int, optional) ‚Äì Number of blocked connections from input channels to output channels. Default: 1\n",
    "\n",
    "* bias (bool, optional) ‚Äì If True, adds a learnable bias to the output. Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# With square kernels and equal stride\n",
    "m = nn.Conv2d(16, 33, 3, stride=2)  # in_channels=16, out_channels=33, kernel_size(filter_size)=3,\n",
    "                                    # stride=2\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "\n",
    "# non-square kernels and unequal stride and with padding and dilation\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2,1), padding=(4, 2), dilation=(3, 1))\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 26, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Q1. How to calculate the output size after Conv2d in PyTorch\n",
    "# https://discuss.pytorch.org/t/how-to-calculate-the-output-size-after-conv2d-in-pytorch/20405\n",
    "\n",
    "inputs = torch.rand(1, 1, 10, 10)\n",
    "mod = nn.Conv2d(1, 32, 3, 2, 1)\n",
    "out = mod(inputs)\n",
    "print(out.shape)  # Ïñ¥Îñ§ Í∞íÏù¥ Ï∂úÎ†•Îê†Íπå?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
