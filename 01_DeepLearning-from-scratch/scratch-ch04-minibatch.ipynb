{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning-from-Scratch Ch04\n",
    "## 4.2.3 mini-batch\n",
    "* mnibatch\n",
    "* 훈련데이터에서 지정한 수의 데이터를 무작위로 골라내는 코드를 작성해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5d1d7ba34394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 수정한 코드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "# sys.patah.append(os.)\n",
    "# 상위 폴더 경로 추가하기\n",
    "# https://seongkyun.github.io/others/2019/04/29/python_import/import numpy as np\n",
    "\n",
    "# 기존 코드\n",
    "# from dataset.mnist import load_mnist\n",
    "\n",
    "# 수정한 코드\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(dataset))))))\n",
    "from mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 불러오고 싶어도 불러올 수 없는 소인의 마음을 아십니까 ㅠㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No.1 절대경로로 파일 불러오기!\n",
    "* 성공\n",
    "* 그런데 매번 이렇게 절대 경로 입력해서 불러오게?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "# print(os.getcwd())\n",
    "os.chdir(\"C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master\")\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.path.dirname사용해서 디렉터리 읽기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path = \"C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master\"\n",
    "full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "os.path.dirname(full_path) # 경로중 디렉터리 명만 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 중 디렉터리 명만 얻을 경우, \n",
    "# 경로의 맨 마지막 path를 파일로 인식하고 그 전 디렉터리까지 읽어온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결로중 디렉터리명(절대경로(경로중 디렉터리명))\n",
    "\n",
    "# 첫 단계 절대경로(경로중 디렉터리명)\n",
    "os.path.abspath(os.path.dirname(full_path))  # os.path.abspath\n",
    "                                             # 특정 경로에 대해 절대 경로 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 번째 단계 결로중 디렉터리명(절대경로(경로중 디렉터리명))\n",
    "\n",
    "os.path.dirname(os.path.abspath(os.path.dirname(full_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계 상위폴더 경로 추가하기\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(full_path))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tutorial은 여기까지!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실전 하위 디렉터리 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 좀 더 Fancy하게 데이터를 불러오는 경로를 지정해 줄 수는 없을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.dirname(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(os.path.dirname(full_path)))  # 디렉터리 변경, 새 시대에 주고온다!\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # train set\n",
    "print(t_train.shape)  # train label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to the batch\n",
    "# load_mnist함수는 MNIST 데이터셋을 읽어오는 함수이다.\n",
    "# 오늘 batch녀석과 MNIST Train만큼은 끝내보자!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MNIST데이터를 읽은 결과, 훈련 데이터는 60,000개고, 입력 데이터는 784열인 이미지 데이터임을 알 수 있다.\n",
    "* 정답 레이블은 10줄짜리 데이터이다. \n",
    "* 따라서 x_train과 t_train의 모습은 각각 (60000, 784)와 (60000, 10)이 된다.\n",
    "* 이 훈련 데이터에서 무작위로 10장만 빼내려면 어떻게 해야할까?\n",
    "* numpy의 np.random.choice()함수를 사용해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41052 19577  5942  1213 50751  7651 36120 49879 35798 29761]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10  # 훈련 데이터에서 임의로 10장을 빼내기 위해 batch 사이즈를 정해준다.\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)  # train데이터에서 임의의 10장에 대한 인덱스를 빼온다\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "                              # 이렇게 하면 x_batch와 t_batch로 임의의 값 10개를 뽑을 수 있다...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이처럼 무작위로 선택한 인덱스를 사용해 미니배치를 뽑아내기만 하면 된다.\n",
    "# 손실함수도 미니배치로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그럼, 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차는 어떻게 구현할까?\n",
    "* 다행히도 조금 전에 구현한 교차 엔트로피 오차(데이터를 하나씩 처리하는 구현)을 조금만 바꿔주면 된다.\n",
    "* 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):  # y는 신경망의 출력\n",
    "                                # t는 정답 레이블이다\n",
    "    if y.ndim == 1:  # 신경망 출력의 차원이 1차원일때를 말하는걸까?\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y.ndim == 1\n",
    "* 신경망 출력의 차원이 1라는 것의 의미는 무엇일까?!?!\n",
    "* 데이터 하나당 CEE(교차 엔트로피 오차)를 구하는 경우는 reshape함수로 데이터의 형상을 바꿔준다.\n",
    "* 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  정답 레이블이 원-핫 인코딩이 아니라 '2', '7'등의 숫자 레이블로 주어졌을 때의 교차엔트로피는 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 숫자 레이블로 주어졌을 떄\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. 상위 디렉터리에 있는 파일은 어떻게 읽어오게? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'A state-of-the-art survey of malware detection approaches using data mining techniques.pdf',\n",
       " 'deep_learning_scratch_master',\n",
       " 'hello.py',\n",
       " 'scratch-ch04-minibatch.ipynb',\n",
       " 'scratch-ch04-minibatch2.ipynb',\n",
       " 'scratch-ch04-NeuralNet.ipynb',\n",
       " 'TED_30_Days_Challenges.pdf']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.dirname(os.path.dirname(full_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = os.path.dirname(os.path.dirname(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "A state-of-the-art survey of malware detection approaches using data mining techniques.pdf\n",
      "deep_learning_scratch_master\n",
      "hello.py\n",
      "scratch-ch04-minibatch.ipynb\n",
      "scratch-ch04-minibatch2.ipynb\n",
      "scratch-ch04-NeuralNet.ipynb\n",
      "TED_30_Days_Challenges.pdf\n"
     ]
    }
   ],
   "source": [
    "## 파이썬 디렉토리 하위 검색하기\n",
    "\n",
    "import os\n",
    "\n",
    "def search(dir):\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        \n",
    "search(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\A state-of-the-art survey of malware detection approaches using data mining techniques.pdf\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\hello.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-minibatch.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-minibatch2.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-NeuralNet.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\TED_30_Days_Challenges.pdf\n"
     ]
    }
   ],
   "source": [
    "# 상위 디렉토리(parent)까지 포함하도록 수정하기\n",
    "\n",
    "import os\n",
    "\n",
    "def search(dir):\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        fullFilename = os.path.join(dir, file)\n",
    "        print(fullFilename)\n",
    "        \n",
    "search(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\\A state-of-the-art survey of malware detection approaches using data mining techniques-checkpoint.pdf\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\\hello-checkpoint.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\\scratch-ch04-minibatch-checkpoint.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\\scratch-ch04-minibatch2-checkpoint.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\.ipynb_checkpoints\\scratch-ch04-NeuralNet-checkpoint.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\A state-of-the-art survey of malware detection approaches using data mining techniques.pdf\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\.gitignore\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\1_vs_2.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\hungry.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\images\\fig 1-1.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\images\\fig 1-2.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\images\\fig 1-3.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\images\\fig 1-4.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\images\\fig 1-5.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\img_show.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\man.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\simple_graph.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\sin_cos_graph.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch01\\sin_graph.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch02\\and_gate.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch02\\nand_gate.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch02\\or_gate.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch02\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch02\\xor_gate.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\mnist_show.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\neuralnet_mnist.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\neuralnet_mnist_batch.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\relu.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\sample_weight.pkl\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\sigmoid.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\sig_step_compare.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch03\\step_function.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\gradient_1d.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\gradient_2d.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\gradient_method.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\gradient_simplenet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\train_neuralnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch04\\two_layer_net.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\buy_apple.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\buy_apple_orange.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\gradient_check.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\layer_naive.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\train_neuralnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch05\\two_layer_net.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\batch_norm_gradient_check.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\batch_norm_test.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\hyperparameter_optimization.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\optimizer_compare_mnist.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\optimizer_compare_naive.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\overfit_dropout.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\overfit_weight_decay.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\weight_init_activation_histogram.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch06\\weight_init_compare.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\apply_filter.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\gradient_check.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\params.pkl\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\simple_convnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\train_convnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch07\\visualize_filter.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\awesome_net.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\deep_convnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\deep_convnet_params.pkl\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\half_float_network.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\misclassified_mnist.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\ch08\\train_deepnet.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\functions.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\gradient.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\layers.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\multi_layer_net.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\multi_layer_net_extend.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\optimizer.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\trainer.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\util.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\common\\__init__.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\cover_image.jpg\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\lena.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\lena_gray.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\mnist.pkl\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\mnist.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\t10k-images-idx3-ubyte.gz\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\t10k-labels-idx1-ubyte.gz\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\train-images-idx3-ubyte.gz\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\train-labels-idx1-ubyte.gz\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\__init__.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\__pycache__\\mnist.cpython-37.pyc\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\dataset\\__pycache__\\__init__.cpython-37.pyc\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\equations_and_figures.zip\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\LICENSE.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\map.png\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master\\README.md\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\deep_learning_scratch_master\\deep-learning-from-scratch-master.zip\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\hello.py\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-minibatch.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-minibatch2.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\scratch-ch04-NeuralNet.ipynb\n",
      "C:\\Users\\stevelee\\Documents\\30-Days-Challenges\\TED_30_Days_Challenges.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def search(dir):\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        fullFilename = os.path.join(dir, file)\n",
    "        if os.path.isdir(fullFilename):\n",
    "            search(fullFilename)\n",
    "        else:\n",
    "            print(fullFilename)\n",
    "    \n",
    "search(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review 20.01.23.Thur.pm5:23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기계학습 - 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다\n",
    "* 빅데이터 수준으로 데이터가 커지게 되면 모든 데이터를 대상으로 일일이 손실함수를 계산하는 것이 현실적이지 않다\n",
    "* 이런경우 데이터의 일부를 추려 전체의 '근사치'로 이용할 수 있다\n",
    "* 즉 신경망의 학습에서 훈련 데이터의 일부만 골라 학습을 수행하는 것이다\n",
    "* 이처럼 훈련데이터의 이룹를 **미니배치(mini-batch)**라고 한다\n",
    "* 60,000장의 MNIST 이미지 데이터에서 100장만 무작위로 뽑아 학습하는 등의 학습방법을 **미니배치 학습 방법**라고 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-53de56837ea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_text, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # (60000, ,784)\n",
    "print(t_train.shape)  # (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\python37.zip', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\DLLs', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib', 'C:\\\\Users\\\\stevelee\\\\Anaconda3', '', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\stevelee\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\stevelee\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 모듈을 불러오는 또다르 방법\n",
    "# https://sshkim.tistory.com/158\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\stevelee\\\\Documents\\\\30-Days-Challenges\\\\deep_learning_scratch_master\\\\deep-learning-from-scratch-master\")  # hard-coding이다\n",
    "                                                                                                                                        # 좀 더 효율적으로 불러올 수는 없을까?\n",
    "# print(sys.path)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)  # (60000, 784)  # x는 훈련데이터\n",
    "print(t_train.shape)  # (60000, 10)   # t는 데이터에 대한 정답 레이블\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mask: [54854 35583 23424 45884 10656 27354 11589 43353 53453 44833]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터에서 무작위로 10장만 빼내려면 어떻게 하면 될까?\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10  # batch size는 10으로 정한다\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print('batch_mask:', batch_mask)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차(CEE)는 어떻게 구현할 수 있을까?\n",
    "* 앞서 구현해 놓은 CEE를 조금만 바꿔주면 된다\n",
    "* 데이터가 하나인 경우와 데이터가 묶어진 경우 모두를 처리할 수 있도록 구현해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:  # 추정값의 차원이 1차원이라면?\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y,size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 구현한 cross_entropy_error를 분석해보자!\n",
    "\n",
    "# 우선 y는 신경망의 출력, t는 정답 레이블이다\n",
    "# y가 1차원이라면 (즉 하나의 데이터에 대해 cee를 구하는 경우라면)\n",
    "# reshape함수로 데이터의 형상을 바꿔준다\n",
    "# 형상을 바꿔준 뒤 배치의 크기로 나눠 정규화 하고 이미지 1장당 CEE를 계산한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 원-핫 인코딩이 아니라 '2'나 '7'등의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다음과 같이 구현할 수 있다\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size), t] + 1e-7)) / batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.5 왜 손실 함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실 함수 외에도 Accuracy 지표가 있지 않는가?\n",
    "* '미분'의 역할에 주목해보자\n",
    "* 신경망의 학습에서는 최적의 매개변수를 탐색할 때 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다\n",
    "* 이 때 매개변수의 미분(정확히는 **기울기**)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_size = t.reshape(1, t.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
