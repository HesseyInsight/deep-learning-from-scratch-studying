{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning-from-Scratch\n",
    "## Chap6. Skills that related to Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 장에서는 신경망 학습의 핵심 개념들을 만나본다.<br/>\n",
    "* 이번 장에서 만나볼 친구들\n",
    "    * 가중치 매개변수의 최적값을 탐색하는 최적화 방법\n",
    "    * 가중치 매개변수의 초깃값\n",
    "    * 하이퍼파라미터 설정 방법\n",
    "    * 이 외에 오버피팅의 대응책인 가중치 감소, 드롭 아웃 등의 정규화 방법들도 살펴본다.\n",
    "    * 배치정규화도 살펴본다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 방법들을 사용하면 신경망(딥러닝) 학습의 효율과 정확도를 높일 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에도 배웠던 적이 있지만 조금 쉬쉬하고 넘겼던 파트들이다...<br/>\n",
    "이번기회에 확실히 개념을 다져두자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 매개변수 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다<br/>\n",
    "이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것으르 **최적화(Optimizatino)**라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안타깝게도 이러한 초최적화는 굉장히 어려운 문제이다(손실 함수의 값을 최대한 낮추는 매개변수를 찾는 과정은 멀고도 험해!)<br/>\n",
    "이는 매개 변수 공간이 매우 넓고 복잡해서 최적의 솔루션을 쉽게 찾지 못하는데 있다. 수식을 풀어 순식간에 최적의 솔루션을 찾는 방법은 없다. 무엇보다도 심층 신경망에서는 매개변수의 수가 엄청나게 많아져서 사태는 더욱 심각해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 SGD(확률적 경사하강법)방법을 사용했다. <br/>\n",
    "매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 몇 번이고 반복해서 점점 최적의 값을 찾아나갔다. <br/>\n",
    "그런데 문제에 따라서는 SGD보다 똑똑한 방법들이 있다. 지금부터는 그러한 방법들에 대해 알아본 후, SGD와는 다른 최적화 기법들을 살펴보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 모험가 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"전설에 나오는 세상에서 가장 깊고 낮은 골짜기, 그곳에 내 모든 보물을 두고왔다\"<br/>\n",
    "모험가는 2가지 제약조건을 가지고 깊은 골짜기를 찾아 나선다.<br/>\n",
    "하나, 지도를 보지 않을 것<br/>\n",
    "둘, 눈가리개를 사용할 것<br/>\n",
    "오직 발바닥으로 전해지는 기울기를 통해 지금 있는 장소에서 가장 기울어진 방향으로 내려가는 것이 바로 모험가의 전략이다.<br/>\n",
    "이는 **SGD**가 취하는 전략과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 구현하기\n",
    "# 나중에 사용할 것을 생각해 클래스의 이름을 SGD라 명명한다.\n",
    "# Again Revuew SGD - 이제 수식의 의미를 완전히 이해할 수 있겠다!\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():  # params까지는 알겠는데... params.key()는 무엇을 의미하는거지?\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 초기화 때 받는 인수 'lr'은 learning rate(학습률)을 뜻한다. <br/>\n",
    "이 학습률을 인스턴스 변수로 유지한다. <br/>\n",
    "<br/>\n",
    "2. update(params, grads) 메서드는 SGD과정에서 반복해서 호출된다. <br/>\n",
    "인수인 params와 grads는 (지금까지 구현했던 것과 마찬가지로) 딕셔너리 변수이다.<br/>\n",
    "params['W1'], grads['W1']등과 같이 각각 가중치 매개변수와 기울기를 저장하고 있다. <br/>\n",
    "딕셔너리 변수의 key값을 하나씩 읽어오는거구나...! params에서 key를 불러오면 매개변수 하나하나 불러올 수 있겠고, 이는 파라미터 업데이트에 사용되겠군!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이런식으로 코드가 작동할 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network = TwoLayerNet(...)   <br>\n",
    "optimizer = SGD()            <br>\n",
    "<br>\n",
    "for i in range(10000):  # epoch은 10000<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_batch, t_batch = get_mini_batch(...)   # 미니배치<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads = network.gradient(x_batch, t_batch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;params = network.params<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.update(params, grads)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크 불러오고, optimizer 지정해주고, epoch만큼 반복하면서 미니배치 학습한다. <br/>\n",
    "optimizer는 최적화를 행하는 자라는 단어인데 이 코드에서는 SGD가 그 역할을 한다<br/>\n",
    "매개변수 갱신은 optimizer가 책임지고 수행하니 우리는 optimizer에 매개변수(params)와 기울기 정보(grads)만 넘겨주면 된다<br/>\n",
    "이처럼 최적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화 하기 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 SGD의 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD는 단순하고 구현도 쉽지만, 문제에 따라서는 비효율적일 때가 있다.<br/>\n",
    "이번 절에서는 SGD의 단점을 알아보고자 다음 함수의 최솟값을 구하는 문제를 생각해보겠다.<br/>\n",
    "f(x, y) = (1/20*x**2) + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고 - 잘 정리된 블로그 \n",
    "    * 수식과 코드로 보는 경사하강법\n",
    "    * https://twinw.tistory.com/247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None  # 인스턴스 변수 v는 물체의 속도이다.\n",
    "                       # 초기화시 아무 값도 담지 않는다.\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}  \n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_likeke(val)  # 인스턴스 변수v는 update()가 처음 호출될 때\n",
    "                                                    # 매개변수와 같은 구조의 데이터를 딕셔너리 변수로 저장한다.\n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum*self.momentum*self.v[key] - self.lr*grads[key]\n",
    "                params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 학습률(수식에서는 에타로 표기) 값이 중요하다.<br/>\n",
    "이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.<br/>\n",
    "이 학습률을 정하는 효과적 기술로 **학습률 감소(learning rate decay)**가 있다.** 이는 학습을 진행하면서 학습률을 점차 줄여가는 방법이다.<br/>\n",
    "처음에는 크게 학습하다가 조금씩 작게 학습한다는 얘기로, 실제 신경망 학습에 자주 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 어느 갱신 방법을 이용할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 가중치의 초깃값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 초깃값을 0으로 하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 은닉층의 활성화값 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 ReLU를 사용할 때의 가중치 초깃값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 배치 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 배치 정규화 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나하나 채워나가보자 20.02.06.Thur pm12:13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
