{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning-from-Scratch\n",
    "## Chap5. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU의 forward()와 backward()함수는 넘파이 배열을 인수로 받는다고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self, x):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):     # forward()에서 x는 넘파이 배열을 인수로 받는다.\n",
    "        self.mask = (x <= 0)  # boolean Indexing: x가 0과 같거나 작은 경우 True를 반환한다.\n",
    "        x[self.mask] = 0      # mask에 해당하는 원소를 0으로 변경해준다. \n",
    "                              # 이는 ReLU함수 적용 결과 0이하의 값에 대해 0으로 반환하는 것과 같다.\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):  # backward()에서 dout은 넘파이 배열을 인수로 받는다.\n",
    "#         print(self.mask)\n",
    "        dout[self.mask] = 0    # mask에 해당하는 원소, 즉 값이 0보다 작은 경우 역전파때 상류의 값이 하류로 흐르지 않는다.\n",
    "        dx = dout              # 반면 순전파 때 x가 0보다 크면 역전파때 상류의 값을 그대로 하류로 흘린다.\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.5],\n",
       "       [ 2. ,  0. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5],\n",
    "              [2.0, 0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_ReLU_forward(): \n",
      "[[1. 0.]\n",
      " [2. 0.]]\n",
      "========================================\n",
      "activation_ReLU_backward(): \n",
      "[[1. 0.]\n",
      " [2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "activation_Relu = ReLU(x)\n",
    "print('activation_ReLU_forward(): ')\n",
    "print(activation_Relu.forward(x))\n",
    "print(\"=\"*40)\n",
    "print('activation_ReLU_backward(): ')\n",
    "print(activation_Relu.backward(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 array x에 들어 있는 값이 0보다 크면(순전파때 x의 값이 0보다 크면) 흘러들어온 gradient가 그대로 흐른다<br/>\n",
    "반면 array x에 들어 있는 값이 0과 같거나 작은 경우(순전파때 x의 값이 0이하인경우) backward시 값이 전달되지 않는다(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여담...<br/>\n",
    "**파이썬 Class 공부를 좀 더 해야겠다** <br/> \n",
    "앞으로 두고두고 많이 사용할 것만 같거든..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 ReLU 계층을 구현하기 위해서는 앞서 배웠던 boolean indexing에 대해 이해를 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid-forward: [[0.73105858 0.88079708]\n",
      " [0.95257413 0.98201379]]\n",
      "==================================================\n",
      "Sigmoid-backward: [[0.19661193 0.20998717]\n",
      " [0.13552998 0.07065082]]\n"
     ]
    }
   ],
   "source": [
    "activation_sigmoid = Sigmoid()\n",
    "print('Sigmoid-forward:',activation_sigmoid.forward(np.array([[1.0, 2.0],[3.0, 4.0]])))\n",
    "print(\"=\"*50)\n",
    "print('Sigmoid-backward:',activation_sigmoid.backward(np.array([[1.0, 2.0],[3.0, 4.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (1 + np.exp(-1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
